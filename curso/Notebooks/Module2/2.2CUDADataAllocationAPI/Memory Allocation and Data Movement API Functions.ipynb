{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "- To learn the basic API functions in CUDA host code\n",
    "  - Device Memory Allocation\n",
    "  - Host-Device Data Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallelism - Vector Addition Example\n",
    "\n",
    "![alt tag](img/3.png)\n",
    "<hr style=\"height:2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Addition – Traditional C Code\n",
    "\n",
    "```cpp\n",
    "\n",
    "// Compute vector sum C = A + B\n",
    "void vecAdd(float *h_A, float *h_B, float *h_C, int n)\n",
    "{\n",
    "int i;\n",
    "for (i = 0; i<n; i++) h_C[i] = h_A[i] + h_B[i];\n",
    "}\n",
    "int main()\n",
    "{\n",
    "// Memory allocation for h_A, h_B, and h_C\n",
    "// I/O to read h_A and h_B, N elements\n",
    "...\n",
    "vecAdd(h_A, h_B, h_C, N);\n",
    "}\n",
    "\n",
    "``` \n",
    "<hr style=\"height:2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous Computing vecAdd CUDA Host Code\n",
    "\n",
    "![alt tag](img/5.png)\n",
    "\n",
    "```cpp\n",
    "\n",
    "#include <cuda.h>\n",
    "void vecAdd(float *h_A, float *h_B, float *h_C, int n)\n",
    "{\n",
    "    int size = n* sizeof(float);\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    \n",
    "    // Part 1\n",
    "    // Allocate device memory for A, B, and C\n",
    "    // copy A and B to device memory\n",
    "    \n",
    "    // Part 2\n",
    "    // Kernel launch code – the device performs the actual vector addition\n",
    "    \n",
    "    // Part 3\n",
    "    // copy C from the device memory\n",
    "    // Free device vectors\n",
    "}\n",
    "\n",
    "```\n",
    "<hr style=\"height:2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Overview of CUDA Memories\n",
    "\n",
    "- Device code can:\n",
    "  - R/W per-thread registers\n",
    "  - R/W all-shared global memory\n",
    "  \n",
    "- Host code can\n",
    "  - Transfer data to/from per grid global memory\n",
    "\n",
    "![alt tag](img/6.png)\n",
    "\n",
    "**We will cover more memory types and more sophisticated memory models later.**\n",
    "\n",
    "\n",
    "<hr style=\"height:2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Device Memory Management API functions\n",
    "\n",
    "\n",
    "![alt tag](img/7.png)\n",
    "<hr style=\"height:2px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<footer>\n",
    "<cite> GPU NVIDIA Teaching Kit - University of Illinois </cite>\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
